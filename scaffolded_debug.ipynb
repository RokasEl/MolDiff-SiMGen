{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ase\n",
    "import numpy as np\n",
    "import ase.io as aio\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rokas/miniconda3/envs/genesis/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from models.bond_predictor import BondPredictor\n",
    "from models.scaffolded_model import ScaffoldedMolDiff\n",
    "from utils.data import traj_to_ase\n",
    "from utils.reconstruct import MolReconsError, reconstruct_from_generated_with_edges\n",
    "from utils.sample import seperate_outputs\n",
    "from utils.transforms import FeaturizeMol, make_data_placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "atoms = aio.read(\"./penicillin_analogues.xyz\", index=\":\")\n",
    "core_ids = np.load(\"./penicillin_core_ids.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "penicillin_core = atoms[0][core_ids[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_pos = penicillin_core.get_positions()-penicillin_core.get_center_of_mass()\n",
    "core_pos = torch.tensor(core_pos, dtype=torch.float32).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1024/1924070459.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"./ckpt/MolDiff.pt\", map_location=\"cuda\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ScaffoldedMolDiff(\n",
       "  (pos_transition): ContigousTransition()\n",
       "  (node_transition): GeneralCategoricalTransition()\n",
       "  (edge_transition): GeneralCategoricalTransition()\n",
       "  (node_embedder): Linear(in_features=8, out_features=246, bias=False)\n",
       "  (edge_embedder): Linear(in_features=6, out_features=54, bias=False)\n",
       "  (time_emb): Sequential(\n",
       "    (0): GaussianSmearing()\n",
       "  )\n",
       "  (denoiser): NodeEdgeNet(\n",
       "    (distance_expansion): GaussianSmearing()\n",
       "    (node_blocks_with_edge): ModuleList(\n",
       "      (0-5): 6 x NodeBlock(\n",
       "        (node_net): MLP(\n",
       "          (net): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (edge_net): MLP(\n",
       "          (net): Sequential(\n",
       "            (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (msg_net): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (gate): MLP(\n",
       "          (net): Sequential(\n",
       "            (0): Linear(in_features=321, out_features=256, bias=True)\n",
       "            (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (centroid_lin): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (act): ReLU()\n",
       "        (out_transform): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (edge_embs): ModuleList(\n",
       "      (0-5): 6 x Linear(in_features=80, out_features=64, bias=True)\n",
       "    )\n",
       "    (edge_blocks): ModuleList(\n",
       "      (0-5): 6 x EdgeBlock(\n",
       "        (bond_ffn_left): BondFFN(\n",
       "          (bond_linear): Linear(in_features=64, out_features=128, bias=False)\n",
       "          (node_linear): Linear(in_features=256, out_features=128, bias=False)\n",
       "          (inter_module): MLP(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (gate): MLP(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=321, out_features=32, bias=True)\n",
       "              (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=32, out_features=64, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (bond_ffn_right): BondFFN(\n",
       "          (bond_linear): Linear(in_features=64, out_features=128, bias=False)\n",
       "          (node_linear): Linear(in_features=256, out_features=128, bias=False)\n",
       "          (inter_module): MLP(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (gate): MLP(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=321, out_features=32, bias=True)\n",
       "              (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=32, out_features=64, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (node_ffn_left): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (node_ffn_right): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (self_ffn): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (out_transform): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (act): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (pos_blocks): ModuleList(\n",
       "      (0-5): 6 x PosUpdate(\n",
       "        (left_lin_edge): MLP(\n",
       "          (net): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "            (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (right_lin_edge): MLP(\n",
       "          (net): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "            (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (edge_lin): BondFFN(\n",
       "          (bond_linear): Linear(in_features=64, out_features=256, bias=False)\n",
       "          (node_linear): Linear(in_features=64, out_features=256, bias=False)\n",
       "          (inter_module): MLP(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (gate): MLP(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=129, out_features=32, bias=True)\n",
       "              (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "              (2): ReLU()\n",
       "              (3): Linear(in_features=32, out_features=1, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (node_decoder): MLP(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): ReLU()\n",
       "      (3): Linear(in_features=256, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (edge_decoder): MLP(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): ReLU()\n",
       "      (3): Linear(in_features=64, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = torch.load(\"./ckpt/MolDiff.pt\", map_location=\"cuda\")\n",
    "train_config = ckpt[\"config\"]\n",
    "featurizer = FeaturizeMol(\n",
    "    train_config.chem.atomic_numbers,\n",
    "    train_config.chem.mol_bond_types,\n",
    "    use_mask_node=train_config.transform.use_mask_node,\n",
    "    use_mask_edge=train_config.transform.use_mask_edge,\n",
    ")\n",
    "model = ScaffoldedMolDiff(\n",
    "    config=train_config.model,\n",
    "    num_node_types=featurizer.num_node_types,\n",
    "    num_edge_types=featurizer.num_edge_types,\n",
    ").to(\"cuda\")\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_numbers = penicillin_core.get_atomic_numbers()\n",
    "core_node_types = [featurizer.ele_to_nodetype[x] for x in core_numbers]\n",
    "core_node_types = torch.tensor(core_node_types, dtype=torch.long).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "mol_size = 40\n",
    "batch_holder = make_data_placeholder(\n",
    "            n_graphs=batch_size, device=\"cuda\", max_size=mol_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:23<00:00, 11.91it/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = model.sample(\n",
    "            n_graphs=batch_size,\n",
    "            batch_node=batch_holder[\"batch_node\"],\n",
    "            halfedge_index=batch_holder[\"halfedge_index\"],\n",
    "            batch_halfedge=batch_holder[\"batch_halfedge\"],\n",
    "            bond_predictor=None,\n",
    "            guidance=None,\n",
    "            scaffold_positions=core_pos,\n",
    "            scaffold_node_types=core_node_types,\n",
    "            readd_noise=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = {\n",
    "            key: [v.cpu().numpy() for v in value] for key, value in outputs.items()\n",
    "        }\n",
    "batch_node = batch_holder[\"batch_node\"].cpu().numpy()\n",
    "halfedge_index = batch_holder[\"halfedge_index\"].cpu().numpy()\n",
    "batch_halfedge = batch_holder[\"batch_halfedge\"].cpu().numpy()\n",
    "output_list = seperate_outputs(\n",
    "    outputs, batch_size, batch_node, halfedge_index, batch_halfedge\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories = []\n",
    "for output_mol in output_list:\n",
    "    traj = traj_to_ase(output_mol[\"traj\"], featurizer, -1)\n",
    "    trajectories.append(traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "aio.write(\"gen_test.xyz\", trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
